{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "manga_stream.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNtoI/r6Nl6sIx1AlpqrtP+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AtlasLearner/manga_stream/blob/main/manga_stream.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL3lm3cJX27a"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnkbhreRcpRP"
      },
      "source": [
        "import re\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGurfXhYZP2A"
      },
      "source": [
        "#!/usr/bin/python\n",
        "import os, sys\n",
        "\n",
        "# Path to be created\n",
        "hpath = '/content/'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wz2otI5Us8VH",
        "outputId": "db7f8d8d-05a1-4324-9770-1d3b9043a34e"
      },
      "source": [
        "# Logging to see requests status\n",
        "import logging\n",
        "\n",
        "import http.client\n",
        " # Set to 1 for debug mode\n",
        "http.client.HTTPConnection.debuglevel = 0\n",
        "\n",
        "# You must initialize logging, otherwise you'll not see debug output.\n",
        "logging.basicConfig()\n",
        "logging.getLogger().setLevel(logging.DEBUG)\n",
        "requests_log = logging.getLogger(\"requests.packages.urllib3\")\n",
        "requests_log.setLevel(logging.DEBUG)\n",
        "requests_log.propagate = False\n",
        "\n",
        "requests.get(\"https://www.google.com\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.google.com:443\n",
            "DEBUG:urllib3.connectionpool:https://www.google.com:443 \"GET / HTTP/1.1\" 200 None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bZb99UhXwAZ"
      },
      "source": [
        "# Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0JXR5Zgcu0z"
      },
      "source": [
        "# Store chapter and respective links\n",
        "chapters = []\n",
        "\n",
        "# Store chapter names that have been downloaded\n",
        "completed = []\n",
        "\n",
        "\n",
        "# One Piece Chapter Link\n",
        "OP_URL  = \"https://onepiecechapters.com/manga/one-piece-chapter-{}/\"\n",
        "BNHA_URL = \"https://w20.readheroacademia.com/manga/boku-no-hero-academia-chapter-{}/.\"\n",
        "\n",
        "# File Naming System\n",
        "OP_CN = \"One_Piece_-_Chapter_\"\n",
        "BNHA_CN = \"Boku_no_Hero_Academia_-_Chapter_\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imuqZXSVXu1d"
      },
      "source": [
        "## Get all img_links using Beautiful Soup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSkUXZsqvlEY"
      },
      "source": [
        "def get_img_links(start, end, url, CN):\n",
        "    print(\"Running\\n\")\n",
        "    for j in range(start, end + 1):\n",
        "        print(\"Current Chapter: \", j)\n",
        "        URL = url.format(j)\n",
        "        print(URL + \"\\n\")\n",
        "        request = requests.get(URL)\n",
        "        if request.status_code == 200:\n",
        "            # requests.get()\n",
        "            #   - If slow\n",
        "            #       1) Use header - Simulates as a website (the intended use) for websites that prevent web scraping\n",
        "            #       2) Use timeout = 5 due to ipv4 vs ipv6\n",
        "            page = requests.get(URL)\n",
        "            \n",
        "            page_content = BeautifulSoup(page.content, 'html.parser')\n",
        "            row_data = [j, CN]\n",
        "            \n",
        "            # Finds all src/links of pictures in <img> tags\n",
        "            for row in page_content.findAll('img'):\n",
        "                link = row.attrs.get(\"src\")\n",
        "                link = re.findall('[^.]ttps://.*jpg|[^.]ttps://.*png', link)\n",
        "                row_data.append(link)\n",
        "\n",
        "            # Removes invalid links based on regex expression\n",
        "            for i,x in enumerate(row_data):\n",
        "                if x==[]:\n",
        "                    row_data.remove([])\n",
        "\n",
        "            chapters.append(row_data)\n",
        "\n",
        "            \n",
        "        else:\n",
        "            print(\"Fail to connect. #FUCKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK\")\n",
        "            break\n",
        "\n",
        "    print(chapters)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EOIyO0XXmQK"
      },
      "source": [
        "## Download all chapters with their respective img links"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3Ar__47LKHz"
      },
      "source": [
        "def dl_chapters():\n",
        "    for j in range(len(chapters)):\n",
        "        print(j)\n",
        "        for i in range(len(chapters[j])):\n",
        "            if i == 0:\n",
        "                chapter_name = str(chapters[j][i+1]) + str(chapters[j][i]) + '_-_'\n",
        "                completed.append(chapter_name)\n",
        "                os.mkdir(chapter_name)\n",
        "                os.chdir(chapter_name)\n",
        "                !pwd\n",
        "                continue\n",
        "            elif i == 1:\n",
        "                continue\n",
        "            else:\n",
        "                Image_URL=''.join(map(str, chapters[j][i]))\n",
        "\n",
        "                filename = re.search(r'/([\\+\\%\\w_-]+[.](jpg|gif|png))$', Image_URL)\n",
        "                if not filename:\n",
        "                    print(\"Regex didn't match with the url: {}\".format(Image_URL))\n",
        "                    continue\n",
        "                \n",
        "                with open('{0:02d}'.format(i-1) + \" - \" + filename.group(1), 'wb') as f:\n",
        "                    response = requests.get(Image_URL)\n",
        "                    if response.status_code == 200:\n",
        "                        f.write(response.content)\n",
        "                        f.close()\n",
        "                    \n",
        "                    else:\n",
        "                    # .group() gives the group based on an index on the regex expression - so 0 is the whole thing, 1 is the group onwards, 2 is (jpg|gif|png)\n",
        "                        print(filename.group(1) + \" - FAILED\")\n",
        "\n",
        "        os.chdir(hpath)\n",
        "        os.system('zip -r /content/' + chapter_name + '.zip ' + chapter_name)\n",
        "        files.download(chapter_name + '.zip')\n",
        "\n",
        "\n",
        "    print(\"Downloaded chapters: \")\n",
        "    print(completed)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUx6i1PiXZJx"
      },
      "source": [
        "## Run the program\n",
        "Remember to change chapter number every week - last updated 22/11/2020\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUQatoIM_j88"
      },
      "source": [
        "# Run this where parameters are number of start of chapter, end of chapter, URL, naming\n",
        "\n",
        "get_img_links(995, 996, OP_URL, OP_CN)\n",
        "get_img_links(260, 280, BNHA_URL, BNHA_CN)\n",
        "\n",
        "dl_chapters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dn6ALvxfdYa9",
        "outputId": "97a1f178-f41c-4dd5-a1ca-c6a40080af18"
      },
      "source": [
        "# Delete everything in the Google Drive Folder\n",
        "\n",
        "# Shell command always run first before the python in-line\n",
        "for i in range(len(completed)):\n",
        "    os.system('rm -rf ' + completed[i])\n",
        "    os.system('rm ' + completed[i] + '.zip')\n",
        "os.chdir(hpath)\n",
        "completed = []\n",
        "chapters = []\n",
        "!ls"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}